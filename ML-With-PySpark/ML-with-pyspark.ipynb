{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d199e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebff97e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-417NR2V:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>flights-ML-pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18fb0060400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"flights-ML-pyspark\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a247",
   "metadata": {},
   "source": [
    "### 1. Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77fc1d",
   "metadata": {},
   "source": [
    "#### Data dictionary FLIGHTS:\n",
    "\n",
    "- mon — month (integer between 1 and 12)\n",
    "- dom — day of month (integer between 1 and 31)\n",
    "- dow — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- carrier — carrier (IATA code)\n",
    "- flight — flight number\n",
    "- org — origin airport (IATA code)\n",
    "- mile — distance (miles)\n",
    "- depart — departure time (decimal hour)\n",
    "- duration — expected duration (minutes)\n",
    "- delay — delay (minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d7a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 275000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('./data/flights-larger.csv',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fceca52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16711"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e9ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_drop_column.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6fe52",
   "metadata": {},
   "source": [
    "### 2. Resolve mising value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f75f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258289\n"
     ]
    }
   ],
   "source": [
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad12d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "flights_km = flights_none_missing.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a093ce2",
   "metadata": {},
   "source": [
    "#### Add columns - String indexer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e4eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|    0.0|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|    0.0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|    2.0|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|    5.0|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|    3.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988f690",
   "metadata": {},
   "source": [
    "### 3. Prediction\n",
    "\n",
    "#### Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8be68211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[10.0,10.0,1.0,2.0,0.0,253.0,8.18,51.0]  |27   |\n",
      "|[11.0,22.0,1.0,2.0,0.0,1188.0,7.17,127.0]|-19  |\n",
      "|[2.0,14.0,5.0,4.0,2.0,3618.0,21.17,365.0]|60   |\n",
      "|[5.0,25.0,3.0,3.0,5.0,621.0,12.92,85.0]  |22   |\n",
      "|[3.0,28.0,1.0,4.0,3.0,1732.0,13.33,182.0]|70   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0671609",
   "metadata": {},
   "source": [
    "#### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5090b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7996856234682855\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights_assembled.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaa018",
   "metadata": {},
   "source": [
    "### Predict using DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67ec67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |1.0       |[0.4933056325023084,0.5066943674976916]|\n",
      "|1    |1.0       |[0.4933056325023084,0.5066943674976916]|\n",
      "|0    |1.0       |[0.3721776229685584,0.6278223770314416]|\n",
      "|1    |1.0       |[0.3721776229685584,0.6278223770314416]|\n",
      "|0    |1.0       |[0.4803883495145631,0.5196116504854369]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7e7e6",
   "metadata": {},
   "source": [
    "### 4. Evaluation model\n",
    "\n",
    "##### A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n",
    "- True Positives (TP) — model predicts positive outcome & known outcome is positive\n",
    "- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n",
    "- False Positives (FP) — model predicts positive outcome but known outcome is negative.\n",
    "##### accuracy = (TN + TP) / (TN + TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f16feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 6776|\n",
      "|    0|       0.0|13459|\n",
      "|    1|       1.0|19339|\n",
      "|    0|       1.0|12165|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.633912522468544\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c49dd5",
   "metadata": {},
   "source": [
    "### 5. Predict using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec5eb362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 9455|\n",
      "|    0|       0.0|14931|\n",
      "|    1|       1.0|16660|\n",
      "|    0|       1.0|10693|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95412042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall    = 0.64\n"
     ]
    }
   ],
   "source": [
    "#Evaluation Model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf9b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|    0.0|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|    0.0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|    2.0|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|    5.0|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|    3.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ac0e7",
   "metadata": {},
   "source": [
    "### 6. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a21594fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c5f877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_pl = flights_km.drop('label')\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_pl.randomSplit([0.8, 0.2], seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b012c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class for creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline = pipeline.fit(flights_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = pipeline.transform(flights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d50cb57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|org_idx|    org_dummy|    dow_dummy|            features|        prediction|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|  0|  1|  2|     AA|JFK|  6.58|     230|   50|2570.0|    2.0|(7,[2],[1.0])|(6,[2],[1.0])|(14,[0,3,10],[257...| 259.3605207555458|\n",
      "|  0|  1|  2|     AA|LGA|   6.0|     240|   45|2235.0|    3.0|(7,[3],[1.0])|(6,[2],[1.0])|(14,[0,4,10],[223...|228.76392636148867|\n",
      "|  0|  1|  2|     AA|LGA|  11.5|     195|  -11|1765.0|    3.0|(7,[3],[1.0])|(6,[2],[1.0])|(14,[0,4,10],[176...| 193.8663278051885|\n",
      "|  0|  1|  2|     AA|LGA| 20.42|     185|   31|1765.0|    3.0|(7,[3],[1.0])|(6,[2],[1.0])|(14,[0,4,10],[176...| 193.8663278051885|\n",
      "|  0|  1|  2|     AA|OGG| 18.08|     420|   -6|5972.0|    7.0|    (7,[],[])|(6,[2],[1.0])|(14,[0,10],[5972....|459.28733222278987|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fa515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5eddd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
