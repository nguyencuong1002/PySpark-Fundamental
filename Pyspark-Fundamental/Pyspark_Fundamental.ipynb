{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c732b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Spark\\\\spark-3.2.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install py4j\n",
    "# !pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f33de184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d80868d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-417NR2V:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spam-email</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x218dcff9c40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"spam-email\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5570f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data = sc.parallelize([2,3,4])\n",
    "simple_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8f4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bee0630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84663f6b",
   "metadata": {},
   "source": [
    "#### Split each email in 'spam' and 'non-spam' RDDs into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38b77884",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_spam = './data/spam.txt'\n",
    "file_path_non_spam = './data/ham.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2257c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element in spam_words is You\n",
      "The first element in non_spam_words is Rofl.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(file_path_spam)\n",
    "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99d41e",
   "metadata": {},
   "source": [
    "#### Transform spam and non spam\n",
    "After splitting the emails into words, our raw data set of 'spam' and 'non-spam' is currently composed of 1-line messages consisting of spam and non-spam messages. In order to classify these messages, we need to convert text into features.\n",
    "\n",
    "In the second part of the exercise, you'll first create a HashingTF() instance to map text to vectors of 200 features, then for each message in 'spam' and 'non-spam' files you'll split them into words, and each word is mapped to one feature. These are the features that will be used to decide whether a message is 'spam' or 'non-spam'. Next, you'll create labels for features. For a valid message, the label will be 0 (i.e. the message is not spam) and for a 'spam' message, the label will be 1 (i.e. the message is spam). Finally, you'll combine both the labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29526d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "# Create a HashingTf instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.union(non_spam_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc21d7",
   "metadata": {},
   "source": [
    "#### Apply model: Logistic Regression model training\n",
    "After creating labels and features for the data, weâ€™re ready to build a model that can learn from it (training). But before you train the model, in this final part of the exercise, you'll split the data into training and test, run Logistic Regression model on the training data, and finally check the accuracy of the model trained on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5219e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy : 0.82\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bbad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
